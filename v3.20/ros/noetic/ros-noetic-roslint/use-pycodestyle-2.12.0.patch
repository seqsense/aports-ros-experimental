diff --git a/CMakeLists.txt b/CMakeLists.txt
index 6f90978..970400a 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -26,4 +26,20 @@ if(CATKIN_ENABLE_TESTING)
     src/roslint/cpplint_wrapper.py
     src/roslint/pycodestyle_wrapper.py)
   roslint_add_test()
+
+  # Run tests in tests/CMakeLists.txt.
+  # add_subdirectory(tests) doesn't work due to
+  # "TARGET 'roslint_roslint' was not created in this directory" error.
+
+  # Some simple Python unit tests, which must return zero status or make
+  # run_tests appears to fail.
+  roslint_python(tests/clean1.py)
+
+  # Run pylint with expected return status:
+  set(_RUNLINT "tests/runlint.py")
+  roslint_custom(${_RUNLINT} "0 pylint tests/clean1.py")
+  roslint_custom(${_RUNLINT} "16 pylint tests/dirty1.py")
+
+  # Make run_tests depend on the roslint targets created above.
+  add_dependencies(run_tests_${PROJECT_NAME} roslint_${PROJECT_NAME})
 endif()
diff --git a/src/roslint/pycodestyle.py b/src/roslint/pycodestyle.py
index b7e0b2f..19f88c2 100644
--- a/src/roslint/pycodestyle.py
+++ b/src/roslint/pycodestyle.py
@@ -25,7 +25,6 @@
 # ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 # CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 # SOFTWARE.
-
 r"""
 Check Python source code formatting, according to PEP 8.
 
@@ -47,9 +46,10 @@ W warnings
 700 statements
 900 syntax error
 """
-from __future__ import with_statement
-
+import bisect
+import configparser
 import inspect
+import io
 import keyword
 import os
 import re
@@ -57,35 +57,25 @@ import sys
 import time
 import tokenize
 import warnings
-import bisect
-
-try:
-    from functools import lru_cache
-except ImportError:
-    def lru_cache(maxsize=128):  # noqa as it's a fake implementation.
-        """Does not really need a real a lru_cache, it's just
-        optimization, so let's just do nothing here. Python 3.2+ will
-        just get better performances, time to upgrade?
-        """
-        return lambda function: function
-
 from fnmatch import fnmatch
+from functools import lru_cache
 from optparse import OptionParser
 
-try:
-    from configparser import RawConfigParser
-    from io import TextIOWrapper
-except ImportError:
-    from ConfigParser import RawConfigParser
+# this is a performance hack.  see https://bugs.python.org/issue43014
+if (
+        sys.version_info < (3, 10) and
+        callable(getattr(tokenize, '_compile', None))
+):  # pragma: no cover (<py310)
+    tokenize._compile = lru_cache(tokenize._compile)  # type: ignore
 
-__version__ = '2.5.0'
+__version__ = '2.12.0'
 
 DEFAULT_EXCLUDE = '.svn,CVS,.bzr,.hg,.git,__pycache__,.tox'
 DEFAULT_IGNORE = 'E121,E123,E126,E226,E24,E704,W503,W504'
 try:
-    if sys.platform == 'win32':
+    if sys.platform == 'win32':  # pragma: win32 cover
         USER_CONFIG = os.path.expanduser(r'~\.pycodestyle')
-    else:
+    else:  # pragma: win32 no cover
         USER_CONFIG = os.path.join(
             os.getenv('XDG_CONFIG_HOME') or os.path.expanduser('~/.config'),
             'pycodestyle'
@@ -94,7 +84,6 @@ except ImportError:
     USER_CONFIG = None
 
 PROJECT_CONFIG = ('setup.cfg', 'tox.ini')
-TESTSUITE_PATH = os.path.join(os.path.dirname(__file__), 'testsuite')
 MAX_LINE_LENGTH = 79
 # Number of blank lines between various code parts.
 BLANK_LINES_CONFIG = {
@@ -104,6 +93,7 @@ BLANK_LINES_CONFIG = {
     'method': 1,
 }
 MAX_DOC_LENGTH = 72
+INDENT_SIZE = 4
 REPORT_FORMAT = {
     'default': '%(path)s:%(row)d:%(col)d: %(code)s %(text)s',
     'pylint': '%(path)s:%(row)d: [%(code)s] %(text)s',
@@ -111,20 +101,15 @@ REPORT_FORMAT = {
 
 PyCF_ONLY_AST = 1024
 SINGLETONS = frozenset(['False', 'None', 'True'])
-KEYWORDS = frozenset(keyword.kwlist + ['print', 'async']) - SINGLETONS
+KEYWORDS = frozenset(keyword.kwlist + ['print']) - SINGLETONS
 UNARY_OPERATORS = frozenset(['>>', '**', '*', '+', '-'])
-ARITHMETIC_OP = frozenset(['**', '*', '/', '//', '+', '-'])
+ARITHMETIC_OP = frozenset(['**', '*', '/', '//', '+', '-', '@'])
 WS_OPTIONAL_OPERATORS = ARITHMETIC_OP.union(['^', '&', '|', '<<', '>>', '%'])
-# Warn for -> function annotation operator in py3.5+ (issue 803)
-FUNCTION_RETURN_ANNOTATION_OP = ['->'] if sys.version_info >= (3, 5) else []
-ASSIGNMENT_EXPRESSION_OP = [':='] if sys.version_info >= (3, 8) else []
 WS_NEEDED_OPERATORS = frozenset([
-    '**=', '*=', '/=', '//=', '+=', '-=', '!=', '<>', '<', '>',
+    '**=', '*=', '/=', '//=', '+=', '-=', '!=', '<', '>',
     '%=', '^=', '&=', '|=', '==', '<=', '>=', '<<=', '>>=', '=',
-    'and', 'in', 'is', 'or'] +
-    FUNCTION_RETURN_ANNOTATION_OP +
-    ASSIGNMENT_EXPRESSION_OP)
-WHITESPACE = frozenset(' \t')
+    'and', 'in', 'is', 'or', '->', ':='])
+WHITESPACE = frozenset(' \t\xa0')
 NEWLINE = frozenset([tokenize.NL, tokenize.NEWLINE])
 SKIP_TOKENS = NEWLINE.union([tokenize.INDENT, tokenize.DEDENT])
 # ERRORTOKEN is triggered by backticks in Python 3
@@ -132,25 +117,27 @@ SKIP_COMMENTS = SKIP_TOKENS.union([tokenize.COMMENT, tokenize.ERRORTOKEN])
 BENCHMARK_KEYS = ['directories', 'files', 'logical lines', 'physical lines']
 
 INDENT_REGEX = re.compile(r'([ \t]*)')
-RAISE_COMMA_REGEX = re.compile(r'raise\s+\w+\s*,')
-RERAISE_COMMA_REGEX = re.compile(r'raise\s+\w+\s*,.*,\s*\w+\s*$')
 ERRORCODE_REGEX = re.compile(r'\b[A-Z]\d{3}\b')
 DOCSTRING_REGEX = re.compile(r'u?r?["\']')
-EXTRANEOUS_WHITESPACE_REGEX = re.compile(r'[\[({] | [\]}),;]| :(?!=)')
+EXTRANEOUS_WHITESPACE_REGEX = re.compile(r'[\[({][ \t]|[ \t][\]}),;:](?!=)')
+WHITESPACE_AFTER_DECORATOR_REGEX = re.compile(r'@\s')
 WHITESPACE_AFTER_COMMA_REGEX = re.compile(r'[,;:]\s*(?:  |\t)')
 COMPARE_SINGLETON_REGEX = re.compile(r'(\bNone|\bFalse|\bTrue)?\s*([=!]=)'
                                      r'\s*(?(1)|(None|False|True))\b')
-COMPARE_NEGATIVE_REGEX = re.compile(r'\b(not)\s+[^][)(}{ ]+\s+(in|is)\s')
-COMPARE_TYPE_REGEX = re.compile(r'(?:[=!]=|is(?:\s+not)?)\s+type(?:s.\w+Type'
-                                r'|\s*\(\s*([^)]*[^ )])\s*\))')
+COMPARE_NEGATIVE_REGEX = re.compile(r'\b(?<!is\s)(not)\s+[^][)(}{ ]+\s+'
+                                    r'(in|is)\s')
+COMPARE_TYPE_REGEX = re.compile(
+    r'[=!]=\s+type(?:\s*\(\s*([^)]*[^\s)])\s*\))'
+    r'|(?<!\.)\btype(?:\s*\(\s*([^)]*[^\s)])\s*\))\s+[=!]='
+)
 KEYWORD_REGEX = re.compile(r'(\s*)\b(?:%s)\b(\s*)' % r'|'.join(KEYWORDS))
-OPERATOR_REGEX = re.compile(r'(?:[^,\s])(\s*)(?:[-+*/|!<=>%&^]+)(\s*)')
+OPERATOR_REGEX = re.compile(r'(?:[^,\s])(\s*)(?:[-+*/|!<=>%&^]+|:=)(\s*)')
 LAMBDA_REGEX = re.compile(r'\blambda\b')
 HUNK_REGEX = re.compile(r'^@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@.*$')
 STARTSWITH_DEF_REGEX = re.compile(r'^(async\s+def|def)\b')
 STARTSWITH_TOP_LEVEL_REGEX = re.compile(r'^(async\s+def\s+|def\s+|class\s+|@)')
 STARTSWITH_INDENT_STATEMENT_REGEX = re.compile(
-    r'^\s*({0})\b'.format('|'.join(s.replace(' ', r'\s+') for s in (
+    r'^\s*({})\b'.format('|'.join(s.replace(' ', r'\s+') for s in (
         'def', 'async def',
         'for', 'async for',
         'if', 'elif', 'else',
@@ -160,19 +147,24 @@ STARTSWITH_INDENT_STATEMENT_REGEX = re.compile(
         'while',
     )))
 )
-DUNDER_REGEX = re.compile(r'^__([^\s]+)__ = ')
+DUNDER_REGEX = re.compile(r"^__([^\s]+)__(?::\s*[a-zA-Z.0-9_\[\]\"]+)? = ")
+BLANK_EXCEPT_REGEX = re.compile(r"except\s*:")
+
+if sys.version_info >= (3, 12):  # pragma: >=3.12 cover
+    FSTRING_START = tokenize.FSTRING_START
+    FSTRING_MIDDLE = tokenize.FSTRING_MIDDLE
+    FSTRING_END = tokenize.FSTRING_END
+else:  # pragma: <3.12 cover
+    FSTRING_START = FSTRING_MIDDLE = FSTRING_END = -1
 
 _checks = {'physical_line': {}, 'logical_line': {}, 'tree': {}}
 
 
 def _get_parameters(function):
-    if sys.version_info >= (3, 3):
-        return [parameter.name
-                for parameter
-                in inspect.signature(function).parameters.values()
-                if parameter.kind == parameter.POSITIONAL_OR_KEYWORD]
-    else:
-        return inspect.getargspec(function)[0]
+    return [parameter.name
+            for parameter
+            in inspect.signature(function).parameters.values()
+            if parameter.kind == parameter.POSITIONAL_OR_KEYWORD]
 
 
 def register_check(check, codes=None):
@@ -211,7 +203,6 @@ def tabs_or_spaces(physical_line, indent_char):
     These options are highly recommended!
 
     Okay: if a == 0:\n    a = 1\n    b = 1
-    E101: if a == 0:\n        a = 1\n\tb = 1
     """
     indent = INDENT_REGEX.match(physical_line).group(1)
     for offset, char in enumerate(indent):
@@ -263,7 +254,7 @@ def trailing_blank_lines(physical_line, lines, line_number, total_lines):
     However the last line should end with a new line (warning W292).
     """
     if line_number == total_lines:
-        stripped_last_line = physical_line.rstrip()
+        stripped_last_line = physical_line.rstrip('\r\n')
         if physical_line and not stripped_last_line:
             return 0, "W391 blank line at end of file"
         if stripped_last_line == physical_line:
@@ -298,12 +289,6 @@ def maximum_line_length(physical_line, max_line_length, multiline,
             (len(chunks) == 2 and chunks[0] == '#')) and \
                 len(line) - len(chunks[-1]) < max_line_length - 7:
             return
-        if hasattr(line, 'decode'):   # Python 2
-            # The line could contain multi-byte characters
-            try:
-                length = len(line.decode('utf-8'))
-            except UnicodeError:
-                pass
         if length > max_line_length:
             return (max_line_length, "E501 line too long "
                     "(%d > %d characters)" % (length, max_line_length))
@@ -314,6 +299,41 @@ def maximum_line_length(physical_line, max_line_length, multiline,
 ########################################################################
 
 
+def _is_one_liner(logical_line, indent_level, lines, line_number):
+    if not STARTSWITH_TOP_LEVEL_REGEX.match(logical_line):
+        return False
+
+    line_idx = line_number - 1
+
+    if line_idx < 1:
+        prev_indent = 0
+    else:
+        prev_indent = expand_indent(lines[line_idx - 1])
+
+    if prev_indent > indent_level:
+        return False
+
+    while line_idx < len(lines):
+        line = lines[line_idx].strip()
+        if not line.startswith('@') and STARTSWITH_TOP_LEVEL_REGEX.match(line):
+            break
+        else:
+            line_idx += 1
+    else:
+        return False  # invalid syntax: EOF while searching for def/class
+
+    next_idx = line_idx + 1
+    while next_idx < len(lines):
+        if lines[next_idx].strip():
+            break
+        else:
+            next_idx += 1
+    else:
+        return True  # line is last in the file
+
+    return expand_indent(lines[next_idx]) <= indent_level
+
+
 @register_check
 def blank_lines(logical_line, blank_lines, indent_level, line_number,
                 blank_before, previous_logical,
@@ -350,7 +370,7 @@ def blank_lines(logical_line, blank_lines, indent_level, line_number,
     top_level_lines = BLANK_LINES_CONFIG['top_level']
     method_lines = BLANK_LINES_CONFIG['method']
 
-    if line_number < top_level_lines + 1 and not previous_logical:
+    if not previous_logical and blank_before < top_level_lines:
         return  # Don't expect blank lines before the first line
     if previous_logical.startswith('@'):
         if blank_lines:
@@ -360,16 +380,11 @@ def blank_lines(logical_line, blank_lines, indent_level, line_number,
           ):
         yield 0, "E303 too many blank lines (%d)" % blank_lines
     elif STARTSWITH_TOP_LEVEL_REGEX.match(logical_line):
-        # If this is a one-liner (i.e. this is not a decorator and the
-        # next line is not more indented), and the previous line is also
-        # not deeper (it would be better to check if the previous line
-        # is part of another def/class at the same level), don't require
-        # blank lines around this.
-        prev_line = lines[line_number - 2] if line_number >= 2 else ''
-        next_line = lines[line_number] if line_number < len(lines) else ''
-        if (not logical_line.startswith("@") and
-                expand_indent(prev_line) <= indent_level and
-                expand_indent(next_line) <= indent_level):
+        # allow a group of one-liners
+        if (
+            _is_one_liner(logical_line, indent_level, lines, line_number) and
+            blank_before == 0
+        ):
             return
         if indent_level:
             if not (blank_before == method_lines or
@@ -383,15 +398,15 @@ def blank_lines(logical_line, blank_lines, indent_level, line_number,
                 for line in lines[line_number - top_level_lines::-1]:
                     if line.strip() and expand_indent(line) < ancestor_level:
                         ancestor_level = expand_indent(line)
-                        nested = line.lstrip().startswith('def ')
+                        nested = STARTSWITH_DEF_REGEX.match(line.lstrip())
                         if nested or ancestor_level == 0:
                             break
                 if nested:
                     yield 0, "E306 expected %s blank line before a " \
                         "nested definition, found 0" % (method_lines,)
                 else:
-                    yield 0, "E301 expected %s blank line, found 0" % (
-                        method_lines,)
+                    yield 0, "E301 expected {} blank line, found 0".format(
+                        method_lines)
         elif blank_before != top_level_lines:
             yield 0, "E302 expected %s blank lines, found %d" % (
                 top_level_lines, blank_before)
@@ -424,18 +439,24 @@ def extraneous_whitespace(logical_line):
     E203: if x == 4: print x, y; x, y = y , x
     E203: if x == 4: print x, y ; x, y = y, x
     E203: if x == 4 : print x, y; x, y = y, x
+
+    Okay: @decorator
+    E204: @ decorator
     """
     line = logical_line
     for match in EXTRANEOUS_WHITESPACE_REGEX.finditer(line):
         text = match.group()
         char = text.strip()
         found = match.start()
-        if text == char + ' ':
+        if text[-1].isspace():
             # assert char in '([{'
             yield found + 1, "E201 whitespace after '%s'" % char
         elif line[found - 1] != ',':
             code = ('E202' if char in '}])' else 'E203')  # if char in ',;:'
-            yield found, "%s whitespace before '%s'" % (code, char)
+            yield found, f"{code} whitespace before '{char}'"
+
+    if WHITESPACE_AFTER_DECORATOR_REGEX.match(logical_line):
+        yield 1, "E204 whitespace after decorator '@'"
 
 
 @register_check
@@ -463,57 +484,33 @@ def whitespace_around_keywords(logical_line):
 
 
 @register_check
-def missing_whitespace_after_import_keyword(logical_line):
-    r"""Multiple imports in form from x import (a, b, c) should have
-    space between import statement and parenthesised name list.
+def missing_whitespace_after_keyword(logical_line, tokens):
+    r"""Keywords should be followed by whitespace.
 
     Okay: from foo import (bar, baz)
     E275: from foo import(bar, baz)
     E275: from importable.module import(bar, baz)
+    E275: if(foo): bar
     """
-    line = logical_line
-    indicator = ' import('
-    if line.startswith('from '):
-        found = line.find(indicator)
-        if -1 < found:
-            pos = found + len(indicator) - 1
-            yield pos, "E275 missing whitespace after keyword"
-
-
-@register_check
-def missing_whitespace(logical_line):
-    r"""Each comma, semicolon or colon should be followed by whitespace.
-
-    Okay: [a, b]
-    Okay: (3,)
-    Okay: a[1:4]
-    Okay: a[:4]
-    Okay: a[1:]
-    Okay: a[1:4:2]
-    E231: ['a','b']
-    E231: foo(bar,baz)
-    E231: [{'a':'b'}]
-    """
-    line = logical_line
-    for index in range(len(line) - 1):
-        char = line[index]
-        next_char = line[index + 1]
-        if char in ',;:' and next_char not in WHITESPACE:
-            before = line[:index]
-            if char == ':' and before.count('[') > before.count(']') and \
-                    before.rfind('{') < before.rfind('['):
-                continue  # Slice syntax, no space required
-            if char == ',' and next_char == ')':
-                continue  # Allow tuple with only one element: (3,)
-            if char == ':' and next_char == '=' and sys.version_info >= (3, 8):
-                continue  # Allow assignment expression
-            yield index, "E231 missing whitespace after '%s'" % char
+    for tok0, tok1 in zip(tokens, tokens[1:]):
+        # This must exclude the True/False/None singletons, which can
+        # appear e.g. as "if x is None:", and async/await, which were
+        # valid identifier names in old Python versions.
+        if (tok0.end == tok1.start and
+                tok0.type == tokenize.NAME and
+                keyword.iskeyword(tok0.string) and
+                tok0.string not in SINGLETONS and
+                not (tok0.string == 'except' and tok1.string == '*') and
+                not (tok0.string == 'yield' and tok1.string == ')') and
+                tok1.string not in ':\n'):
+            yield tok0.end, "E275 missing whitespace after keyword"
 
 
 @register_check
 def indentation(logical_line, previous_logical, indent_char,
-                indent_level, previous_indent_level):
-    r"""Use 4 spaces per indentation level.
+                indent_level, previous_indent_level,
+                indent_size):
+    r"""Use indent_size (PEP8 says 4) spaces per indentation level.
 
     For really old code that you don't want to mess up, you can continue
     to use 8-space tabs.
@@ -533,8 +530,11 @@ def indentation(logical_line, previous_logical, indent_char,
     """
     c = 0 if logical_line else 3
     tmpl = "E11%d %s" if logical_line else "E11%d %s (comment)"
-    if indent_level % 4:
-        yield 0, tmpl % (1 + c, "indentation is not a multiple of four")
+    if indent_level % indent_size:
+        yield 0, tmpl % (
+            1 + c,
+            "indentation is not a multiple of " + str(indent_size),
+        )
     indent_expect = previous_logical.endswith(':')
     if indent_expect and indent_level <= previous_indent_level:
         yield 0, tmpl % (2 + c, "expected an indented block")
@@ -550,7 +550,7 @@ def indentation(logical_line, previous_logical, indent_char,
 
 @register_check
 def continued_indentation(logical_line, tokens, indent_level, hang_closing,
-                          indent_char, noqa, verbose):
+                          indent_char, indent_size, noqa, verbose):
     r"""Continuation lines indentation.
 
     Continuation lines should align wrapped elements either vertically
@@ -589,7 +589,8 @@ def continued_indentation(logical_line, tokens, indent_level, hang_closing,
     indent_next = logical_line.endswith(':')
 
     row = depth = 0
-    valid_hangs = (4,) if indent_char != '\t' else (4, 8)
+    valid_hangs = (indent_size,) if indent_char != '\t' \
+        else (indent_size, indent_size * 2)
     # remember how many brackets were opened on each line
     parens = [0] * nrows
     # relative indents of physical lines
@@ -654,7 +655,8 @@ def continued_indentation(logical_line, tokens, indent_level, hang_closing,
                     # visual indent is broken
                     yield (start, "E128 continuation line "
                            "under-indented for visual indent")
-            elif hanging_indent or (indent_next and rel_indent[row] == 8):
+            elif hanging_indent or (indent_next and
+                                    rel_indent[row] == 2 * indent_size):
                 # hanging indent is verified
                 if close_bracket and not hang_closing:
                     yield (start, "E123 closing bracket does not match "
@@ -677,7 +679,7 @@ def continued_indentation(logical_line, tokens, indent_level, hang_closing,
                     error = "E131", "unaligned for hanging indent"
                 else:
                     hangs[depth] = hang
-                    if hang > 4:
+                    if hang > indent_size:
                         error = "E126", "over-indented for hanging indent"
                     else:
                         error = "E121", "under-indented for hanging indent"
@@ -690,11 +692,13 @@ def continued_indentation(logical_line, tokens, indent_level, hang_closing,
             indent[depth] = start[1]
             indent_chances[start[1]] = True
             if verbose >= 4:
-                print("bracket depth %s indent to %s" % (depth, start[1]))
+                print(f"bracket depth {depth} indent to {start[1]}")
         # deal with implicit string concatenation
-        elif (token_type in (tokenize.STRING, tokenize.COMMENT) or
-              text in ('u', 'ur', 'b', 'br')):
+        elif token_type in (tokenize.STRING, tokenize.COMMENT, FSTRING_START):
             indent_chances[start[1]] = str
+        # visual indent after assert/raise/with
+        elif not row and not depth and text in ["assert", "raise", "with"]:
+            indent_chances[end[1] + 1] = True
         # special case for the "if" statement because len("if (") == 4
         elif not indent_chances and not row and not depth and text == 'if':
             indent_chances[end[1] + 1] = True
@@ -741,8 +745,8 @@ def continued_indentation(logical_line, tokens, indent_level, hang_closing,
         if last_token_multiline:
             rel_indent[end[0] - first_row] = rel_indent[row]
 
-    if indent_next and expand_indent(line) == indent_level + 4:
-        pos = (start[0], indent[0] + 4)
+    if indent_next and expand_indent(line) == indent_level + indent_size:
+        pos = (start[0], indent[0] + indent_size)
         if visual_indent:
             code = "E129 visually indented line"
         else:
@@ -769,14 +773,22 @@ def whitespace_before_parameters(logical_line, tokens):
     prev_type, prev_text, __, prev_end, __ = tokens[0]
     for index in range(1, len(tokens)):
         token_type, text, start, end, __ = tokens[index]
-        if (token_type == tokenize.OP and
+        if (
+            token_type == tokenize.OP and
             text in '([' and
             start != prev_end and
             (prev_type == tokenize.NAME or prev_text in '}])') and
             # Syntax "class A (B):" is allowed, but avoid it
             (index < 2 or tokens[index - 2][1] != 'class') and
-                # Allow "return (a.foo for a in range(5))"
-                not keyword.iskeyword(prev_text)):
+            # Allow "return (a.foo for a in range(5))"
+            not keyword.iskeyword(prev_text) and
+            (
+                sys.version_info < (3, 9) or
+                # 3.12+: type is a soft keyword but no braces after
+                prev_text == 'type' or
+                not keyword.issoftkeyword(prev_text)
+            )
+        ):
             yield prev_end, "E211 whitespace before '%s'" % text
         prev_type = token_type
         prev_text = text
@@ -808,14 +820,16 @@ def whitespace_around_operator(logical_line):
 
 
 @register_check
-def missing_whitespace_around_operator(logical_line, tokens):
-    r"""Surround operators with a single space on either side.
+def missing_whitespace(logical_line, tokens):
+    r"""Surround operators with the correct amount of whitespace.
 
     - Always surround these binary operators with a single space on
       either side: assignment (=), augmented assignment (+=, -= etc.),
       comparisons (==, <, >, !=, <=, >=, in, not in, is, is not),
       Booleans (and, or, not).
 
+    - Each comma, semicolon or colon should be followed by whitespace.
+
     - If operators with different priorities are used, consider adding
       whitespace around the operators with the lowest priorities.
 
@@ -826,6 +840,13 @@ def missing_whitespace_around_operator(logical_line, tokens):
     Okay: c = (a + b) * (a - b)
     Okay: foo(bar, key='word', *args, **kwargs)
     Okay: alpha[:-i]
+    Okay: [a, b]
+    Okay: (3,)
+    Okay: a[3,] = 1
+    Okay: a[1:4]
+    Okay: a[:4]
+    Okay: a[1:]
+    Okay: a[1:4:2]
 
     E225: i=i+1
     E225: submitted +=1
@@ -836,19 +857,52 @@ def missing_whitespace_around_operator(logical_line, tokens):
     E226: hypot2 = x*x + y*y
     E227: c = a|b
     E228: msg = fmt%(errno, errmsg)
+    E231: ['a','b']
+    E231: foo(bar,baz)
+    E231: [{'a':'b'}]
     """
-    parens = 0
     need_space = False
     prev_type = tokenize.OP
     prev_text = prev_end = None
     operator_types = (tokenize.OP, tokenize.NAME)
+    brace_stack = []
     for token_type, text, start, end, line in tokens:
+        if token_type == tokenize.OP and text in {'[', '(', '{'}:
+            brace_stack.append(text)
+        elif token_type == FSTRING_START:  # pragma: >=3.12 cover
+            brace_stack.append('f')
+        elif token_type == tokenize.NAME and text == 'lambda':
+            brace_stack.append('l')
+        elif brace_stack:
+            if token_type == tokenize.OP and text in {']', ')', '}'}:
+                brace_stack.pop()
+            elif token_type == FSTRING_END:  # pragma: >=3.12 cover
+                brace_stack.pop()
+            elif (
+                    brace_stack[-1] == 'l' and
+                    token_type == tokenize.OP and
+                    text == ':'
+            ):
+                brace_stack.pop()
+
         if token_type in SKIP_COMMENTS:
             continue
-        if text in ('(', 'lambda'):
-            parens += 1
-        elif text == ')':
-            parens -= 1
+
+        if token_type == tokenize.OP and text in {',', ';', ':'}:
+            next_char = line[end[1]:end[1] + 1]
+            if next_char not in WHITESPACE and next_char not in '\r\n':
+                # slice
+                if text == ':' and brace_stack[-1:] == ['[']:
+                    pass
+                # 3.12+ fstring format specifier
+                elif text == ':' and brace_stack[-2:] == ['f', '{']:  # pragma: >=3.12 cover  # noqa: E501
+                    pass
+                # tuple (and list for some reason?)
+                elif text == ',' and next_char in ')]':
+                    pass
+                else:
+                    yield start, f'E231 missing whitespace after {text!r}'
+
         if need_space:
             if start != prev_end:
                 # Found a (probably) needed space
@@ -856,11 +910,18 @@ def missing_whitespace_around_operator(logical_line, tokens):
                     yield (need_space[0],
                            "E225 missing whitespace around operator")
                 need_space = False
-            elif text == '>' and prev_text in ('<', '-'):
-                # Tolerate the "<>" operator, even if running Python 3
-                # Deal with Python 3's annotated return value "->"
-                pass
-            elif prev_text == '/' and text == ',':
+            elif (
+                    # def f(a, /, b):
+                    #           ^
+                    # def f(a, b, /):
+                    #              ^
+                    # f = lambda a, /:
+                    #                ^
+                    prev_text == '/' and text in {',', ')', ':'} or
+                    # def f(a, b, /):
+                    #               ^
+                    prev_text == ')' and text == ':'
+            ):
                 # Tolerate the "/" operator in function definition
                 # For more info see PEP570
                 pass
@@ -878,8 +939,16 @@ def missing_whitespace_around_operator(logical_line, tokens):
                            "around %s operator" % (code, optype))
                 need_space = False
         elif token_type in operator_types and prev_end is not None:
-            if text == '=' and parens:
-                # Allow keyword args or defaults: foo(bar=None).
+            if (
+                    text == '=' and (
+                        # allow lambda default args: lambda x=None: None
+                        brace_stack[-1:] == ['l'] or
+                        # allow keyword args or defaults: foo(bar=None).
+                        brace_stack[-1:] == ['('] or
+                        # allow python 3.8 fstring repr specifier
+                        brace_stack[-2:] == ['f', '{']
+                    )
+            ):
                 pass
             elif text in WS_NEEDED_OPERATORS:
                 need_space = True
@@ -887,8 +956,13 @@ def missing_whitespace_around_operator(logical_line, tokens):
                 # Check if the operator is used as a binary operator
                 # Allow unary operators: -123, -x, +1.
                 # Allow argument unpacking: foo(*args, **kwargs).
-                if (prev_text in '}])' if prev_type == tokenize.OP
-                        else prev_text not in KEYWORDS):
+                if prev_type == tokenize.OP and prev_text in '}])' or (
+                    prev_type != tokenize.OP and
+                    prev_text not in KEYWORDS and (
+                        sys.version_info < (3, 9) or
+                        not keyword.issoftkeyword(prev_text)
+                    )
+                ):
                     need_space = None
             elif text in WS_OPTIONAL_OPERATORS:
                 need_space = None
@@ -993,21 +1067,24 @@ def whitespace_around_named_parameter_equals(logical_line, tokens):
 
 @register_check
 def whitespace_before_comment(logical_line, tokens):
-    r"""Separate inline comments by at least two spaces.
+    """Separate inline comments by at least two spaces.
 
     An inline comment is a comment on the same line as a statement.
     Inline comments should be separated by at least two spaces from the
     statement. They should start with a # and a single space.
 
-    Each line of a block comment starts with a # and a single space
-    (unless it is indented text inside the comment).
+    Each line of a block comment starts with a # and one or multiple
+    spaces as there can be indented text inside the comment.
 
     Okay: x = x + 1  # Increment x
     Okay: x = x + 1    # Increment x
-    Okay: # Block comment
+    Okay: # Block comments:
+    Okay: #  - Block comment list
+    Okay: # \xa0- Block comment list
     E261: x = x + 1 # Increment x
     E262: x = x + 1  #Increment x
     E262: x = x + 1  #  Increment x
+    E262: x = x + 1  # \xa0Increment x
     E265: #Block comment
     E266: ### Block comment
     """
@@ -1065,10 +1142,6 @@ def module_imports_on_top_of_file(
     Okay: # this is a comment\nimport os
     Okay: '''this is a module docstring'''\nimport os
     Okay: r'''this is a module docstring'''\nimport os
-    Okay:
-    try:\n\timport x\nexcept ImportError:\n\tpass\nelse:\n\tpass\nimport y
-    Okay:
-    try:\n\timport x\nexcept ImportError:\n\tpass\nfinally:\n\tpass\nimport y
     E402: a=1\nimport os
     E402: 'One string'\n"Two string"\nimport os
     E402: a=1\nfrom sys import x
@@ -1149,15 +1222,16 @@ def compound_statements(logical_line):
     counts = {char: 0 for char in '{}[]()'}
     while -1 < found < last_char:
         update_counts(line[prev_found:found], counts)
-        if ((counts['{'] <= counts['}'] and   # {'a': 1} (dict)
-             counts['['] <= counts[']'] and   # [1:2] (slice)
-             counts['('] <= counts[')']) and  # (annotation)
-            not (sys.version_info >= (3, 8) and
-                 line[found + 1] == '=')):  # assignment expression
+        if (
+                counts['{'] <= counts['}'] and  # {'a': 1} (dict)
+                counts['['] <= counts[']'] and  # [1:2] (slice)
+                counts['('] <= counts[')'] and  # (annotation)
+                line[found + 1] != '='  # assignment expression
+        ):
             lambda_kw = LAMBDA_REGEX.search(line, 0, found)
             if lambda_kw:
                 before = line[:lambda_kw.start()].rstrip()
-                if before[-1:] == '=' and isidentifier(before[:-1].strip()):
+                if before[-1:] == '=' and before[:-1].strip().isidentifier():
                     yield 0, ("E731 do not assign a lambda expression, use a "
                               "def")
                 break
@@ -1202,6 +1276,8 @@ def explicit_line_join(logical_line, tokens):
             comment = True
         if start[0] != prev_start and parens and backslash and not comment:
             yield backslash, "E502 the backslash is redundant between brackets"
+        if start[0] != prev_start:
+            comment = False  # Reset comment flag on newline
         if end[0] != prev_end:
             if line.rstrip('\r\n').endswith('\\'):
                 backslash = (end[0], len(line.splitlines()[-1]) - 1)
@@ -1217,20 +1293,19 @@ def explicit_line_join(logical_line, tokens):
                 parens -= 1
 
 
+# The % character is strictly speaking a binary operator, but the
+# common usage seems to be to put it next to the format parameters,
+# after a line break.
 _SYMBOLIC_OPS = frozenset("()[]{},:.;@=%~") | frozenset(("...",))
 
 
 def _is_binary_operator(token_type, text):
-    is_op_token = token_type == tokenize.OP
-    is_conjunction = text in ['and', 'or']
-    # NOTE(sigmavirus24): Previously the not_a_symbol check was executed
-    # conditionally. Since it is now *always* executed, text may be
-    # None. In that case we get a TypeError for `text not in str`.
-    not_a_symbol = text and text not in _SYMBOLIC_OPS
-    # The % character is strictly speaking a binary operator, but the
-    # common usage seems to be to put it next to the format parameters,
-    # after a line break.
-    return ((is_op_token or is_conjunction) and not_a_symbol)
+    return (
+        token_type == tokenize.OP or
+        text in {'and', 'or'}
+    ) and (
+        text not in _SYMBOLIC_OPS
+    )
 
 
 def _break_around_binary_operators(tokens):
@@ -1342,8 +1417,10 @@ def comparison_to_singleton(logical_line, noqa):
     None was set to some other value.  The other value might have a type
     (such as a container) that could be false in a boolean context!
     """
-    match = not noqa and COMPARE_SINGLETON_REGEX.search(logical_line)
-    if match:
+    if noqa:
+        return
+
+    for match in COMPARE_SINGLETON_REGEX.finditer(logical_line):
         singleton = match.group(1) or match.group(3)
         same = (match.group(2) == '==')
 
@@ -1383,26 +1460,24 @@ def comparison_negative(logical_line):
 
 @register_check
 def comparison_type(logical_line, noqa):
-    r"""Object type comparisons should always use isinstance().
+    r"""Object type comparisons should `is` / `is not` / `isinstance()`.
 
     Do not compare types directly.
 
     Okay: if isinstance(obj, int):
-    E721: if type(obj) is type(1):
-
-    When checking if an object is a string, keep in mind that it might
-    be a unicode string too! In Python 2.3, str and unicode have a
-    common base class, basestring, so you can do:
-
-    Okay: if isinstance(obj, basestring):
-    Okay: if type(a1) is type(b1):
+    Okay: if type(obj) is int:
+    E721: if type(obj) == type(1):
     """
     match = COMPARE_TYPE_REGEX.search(logical_line)
     if match and not noqa:
         inst = match.group(1)
-        if inst and isidentifier(inst) and inst not in SINGLETONS:
+        if inst and inst.isidentifier() and inst not in SINGLETONS:
             return  # Allow comparison for types which are not obvious
-        yield match.start(), "E721 do not compare types, use 'isinstance()'"
+        yield (
+            match.start(),
+            "E721 do not compare types, for exact checks use `is` / `is not`, "
+            "for instance checks use `isinstance()`",
+        )
 
 
 @register_check
@@ -1417,8 +1492,7 @@ def bare_except(logical_line, noqa):
     if noqa:
         return
 
-    regex = re.compile(r"except\s*:")
-    match = regex.match(logical_line)
+    match = BLANK_EXCEPT_REGEX.match(logical_line)
     if match:
         yield match.start(), "E722 do not use bare 'except'"
 
@@ -1438,14 +1512,19 @@ def ambiguous_identifier(logical_line, tokens):
     E741: I = 42
 
     Variables can be bound in several other contexts, including class
-    and function definitions, 'global' and 'nonlocal' statements,
-    exception handlers, and 'with' and 'for' statements.
+    and function definitions, lambda functions, 'global' and 'nonlocal'
+    statements, exception handlers, and 'with' and 'for' statements.
     In addition, we have a special handling for function parameters.
 
     Okay: except AttributeError as o:
     Okay: with lock as L:
     Okay: foo(l=12)
+    Okay: foo(l=I)
     Okay: for a in foo(l=12):
+    Okay: lambda arg: arg * l
+    Okay: lambda a=l[I:5]: None
+    Okay: lambda x=a.I: None
+    Okay: if l >= 12:
     E741: except AttributeError as O:
     E741: with lock as l:
     E741: global I
@@ -1454,32 +1533,39 @@ def ambiguous_identifier(logical_line, tokens):
     E741: def foo(l=12):
     E741: l = foo(l=12)
     E741: for l in range(10):
+    E741: [l for l in lines if l]
+    E741: lambda l: None
+    E741: lambda a=x[1:5], l: None
+    E741: lambda **l:
+    E741: def f(**l):
     E742: class I(object):
     E743: def l(x):
     """
-    is_func_def = False  # Set to true if 'def' is found
-    parameter_parentheses_level = 0
+    func_depth = None  # set to brace depth if 'def' or 'lambda' is found
+    seen_colon = False  # set to true if we're done with function parameters
+    brace_depth = 0
     idents_to_avoid = ('l', 'O', 'I')
     prev_type, prev_text, prev_start, prev_end, __ = tokens[0]
-    for token_type, text, start, end, line in tokens[1:]:
+    for index in range(1, len(tokens)):
+        token_type, text, start, end, line = tokens[index]
         ident = pos = None
         # find function definitions
-        if prev_text == 'def':
-            is_func_def = True
+        if prev_text in {'def', 'lambda'}:
+            func_depth = brace_depth
+            seen_colon = False
+        elif (
+                func_depth is not None and
+                text == ':' and
+                brace_depth == func_depth
+        ):
+            seen_colon = True
         # update parameter parentheses level
-        if parameter_parentheses_level == 0 and \
-                prev_type == tokenize.NAME and \
-                token_type == tokenize.OP and text == '(':
-            parameter_parentheses_level = 1
-        elif parameter_parentheses_level > 0 and \
-                token_type == tokenize.OP:
-            if text == '(':
-                parameter_parentheses_level += 1
-            elif text == ')':
-                parameter_parentheses_level -= 1
+        if text in '([{':
+            brace_depth += 1
+        elif text in ')]}':
+            brace_depth -= 1
         # identifiers on the lhs of an assignment operator
-        if token_type == tokenize.OP and '=' in text and \
-                parameter_parentheses_level == 0:
+        if text == ':=' or (text == '=' and brace_depth == 0):
             if prev_text in idents_to_avoid:
                 ident = prev_text
                 pos = prev_start
@@ -1489,11 +1575,16 @@ def ambiguous_identifier(logical_line, tokens):
             if text in idents_to_avoid:
                 ident = text
                 pos = start
-        # function parameter definitions
-        if is_func_def:
-            if text in idents_to_avoid:
-                ident = text
-                pos = start
+        # function / lambda parameter definitions
+        if (
+                func_depth is not None and
+                not seen_colon and
+                index < len(tokens) - 1 and tokens[index + 1][1] in ':,=)' and
+                prev_text in {'lambda', ',', '*', '**', '('} and
+                text in idents_to_avoid
+        ):
+            ident = text
+            pos = start
         if prev_text == 'class':
             if text in idents_to_avoid:
                 yield start, "E742 ambiguous class definition '%s'" % text
@@ -1502,64 +1593,10 @@ def ambiguous_identifier(logical_line, tokens):
                 yield start, "E743 ambiguous function definition '%s'" % text
         if ident:
             yield pos, "E741 ambiguous variable name '%s'" % ident
-        prev_type = token_type
         prev_text = text
         prev_start = start
 
 
-@register_check
-def python_3000_has_key(logical_line, noqa):
-    r"""The {}.has_key() method is removed in Python 3: use the 'in'
-    operator.
-
-    Okay: if "alph" in d:\n    print d["alph"]
-    W601: assert d.has_key('alph')
-    """
-    pos = logical_line.find('.has_key(')
-    if pos > -1 and not noqa:
-        yield pos, "W601 .has_key() is deprecated, use 'in'"
-
-
-@register_check
-def python_3000_raise_comma(logical_line):
-    r"""When raising an exception, use "raise ValueError('message')".
-
-    The older form is removed in Python 3.
-
-    Okay: raise DummyError("Message")
-    W602: raise DummyError, "Message"
-    """
-    match = RAISE_COMMA_REGEX.match(logical_line)
-    if match and not RERAISE_COMMA_REGEX.match(logical_line):
-        yield match.end() - 1, "W602 deprecated form of raising exception"
-
-
-@register_check
-def python_3000_not_equal(logical_line):
-    r"""New code should always use != instead of <>.
-
-    The older syntax is removed in Python 3.
-
-    Okay: if a != 'no':
-    W603: if a <> 'no':
-    """
-    pos = logical_line.find('<>')
-    if pos > -1:
-        yield pos, "W603 '<>' is deprecated, use '!='"
-
-
-@register_check
-def python_3000_backticks(logical_line):
-    r"""Use repr() instead of backticks in Python 3.
-
-    Okay: val = repr(1 + 2)
-    W604: val = `1 + 2`
-    """
-    pos = logical_line.find('`')
-    if pos > -1:
-        yield pos, "W604 backticks are deprecated, use 'repr()'"
-
-
 @register_check
 def python_3000_invalid_escape_sequence(logical_line, tokens, noqa):
     r"""Invalid escape sequences are deprecated in Python 3.6.
@@ -1592,102 +1629,32 @@ def python_3000_invalid_escape_sequence(logical_line, tokens, noqa):
         'U',
     ]
 
-    for token_type, text, start, end, line in tokens:
-        if token_type == tokenize.STRING:
-            start_line, start_col = start
-            quote = text[-3:] if text[-3:] in ('"""', "'''") else text[-1]
+    prefixes = []
+    for token_type, text, start, _, _ in tokens:
+        if token_type in {tokenize.STRING, FSTRING_START}:
             # Extract string modifiers (e.g. u or r)
-            quote_pos = text.index(quote)
-            prefix = text[:quote_pos].lower()
-            start = quote_pos + len(quote)
-            string = text[start:-len(quote)]
+            prefixes.append(text[:text.index(text[-1])].lower())
 
-            if 'r' not in prefix:
-                pos = string.find('\\')
+        if token_type in {tokenize.STRING, FSTRING_MIDDLE}:
+            if 'r' not in prefixes[-1]:
+                start_line, start_col = start
+                pos = text.find('\\')
                 while pos >= 0:
                     pos += 1
-                    if string[pos] not in valid:
-                        line = start_line + string.count('\n', 0, pos)
+                    if text[pos] not in valid:
+                        line = start_line + text.count('\n', 0, pos)
                         if line == start_line:
-                            col = start_col + len(prefix) + len(quote) + pos
+                            col = start_col + pos
                         else:
-                            col = pos - string.rfind('\n', 0, pos) - 1
+                            col = pos - text.rfind('\n', 0, pos) - 1
                         yield (
                             (line, col - 1),
-                            "W605 invalid escape sequence '\\%s'" %
-                            string[pos],
+                            f"W605 invalid escape sequence '\\{text[pos]}'"
                         )
-                    pos = string.find('\\', pos + 1)
+                    pos = text.find('\\', pos + 1)
 
-
-@register_check
-def python_3000_async_await_keywords(logical_line, tokens):
-    """'async' and 'await' are reserved keywords starting at Python 3.7.
-
-    W606: async = 42
-    W606: await = 42
-    Okay: async def read(db):\n    data = await db.fetch('SELECT ...')
-    """
-    # The Python tokenize library before Python 3.5 recognizes
-    # async/await as a NAME token. Therefore, use a state machine to
-    # look for the possible async/await constructs as defined by the
-    # Python grammar:
-    # https://docs.python.org/3/reference/grammar.html
-
-    state = None
-    for token_type, text, start, end, line in tokens:
-        error = False
-
-        if token_type == tokenize.NL:
-            continue
-
-        if state is None:
-            if token_type == tokenize.NAME:
-                if text == 'async':
-                    state = ('async_stmt', start)
-                elif text == 'await':
-                    state = ('await', start)
-                elif (token_type == tokenize.NAME and
-                      text in ('def', 'for')):
-                    state = ('define', start)
-
-        elif state[0] == 'async_stmt':
-            if token_type == tokenize.NAME and text in ('def', 'with', 'for'):
-                # One of funcdef, with_stmt, or for_stmt. Return to
-                # looking for async/await names.
-                state = None
-            else:
-                error = True
-        elif state[0] == 'await':
-            if token_type == tokenize.NAME:
-                # An await expression. Return to looking for async/await
-                # names.
-                state = None
-            elif token_type == tokenize.OP and text == '(':
-                state = None
-            else:
-                error = True
-        elif state[0] == 'define':
-            if token_type == tokenize.NAME and text in ('async', 'await'):
-                error = True
-            else:
-                state = None
-
-        if error:
-            yield (
-                state[1],
-                "W606 'async' and 'await' are reserved keywords starting with "
-                "Python 3.7",
-            )
-            state = None
-
-    # Last token
-    if state is not None:
-        yield (
-            state[1],
-            "W606 'async' and 'await' are reserved keywords starting with "
-            "Python 3.7",
-        )
+        if token_type in {tokenize.STRING, FSTRING_END}:
+            prefixes.pop()
 
 
 ########################################################################
@@ -1719,12 +1686,6 @@ def maximum_doc_length(logical_line, max_doc_length, noqa, tokens):
             if prev_token is None or prev_token in SKIP_TOKENS:
                 lines = line.splitlines()
                 for line_num, physical_line in enumerate(lines):
-                    if hasattr(physical_line, 'decode'):  # Python 2
-                        # The line could contain multi-byte characters
-                        try:
-                            physical_line = physical_line.decode('utf-8')
-                        except UnicodeError:
-                            pass
                     if start[0] + line_num == 1 and line.startswith('#!'):
                         return
                     length = len(physical_line)
@@ -1750,32 +1711,21 @@ def maximum_doc_length(logical_line, max_doc_length, noqa, tokens):
 ########################################################################
 
 
-if sys.version_info < (3,):
-    # Python 2: implicit encoding.
-    def readlines(filename):
-        """Read the source code."""
-        with open(filename, 'rU') as f:
+def readlines(filename):
+    """Read the source code."""
+    try:
+        with tokenize.open(filename) as f:
             return f.readlines()
-    isidentifier = re.compile(r'[a-zA-Z_]\w*$').match
-    stdin_get_value = sys.stdin.read
-else:
-    # Python 3
-    def readlines(filename):
-        """Read the source code."""
-        try:
-            with open(filename, 'rb') as f:
-                (coding, lines) = tokenize.detect_encoding(f.readline)
-                f = TextIOWrapper(f, coding, line_buffering=True)
-                return [line.decode(coding) for line in lines] + f.readlines()
-        except (LookupError, SyntaxError, UnicodeError):
-            # Fall back if file encoding is improperly declared
-            with open(filename, encoding='latin-1') as f:
-                return f.readlines()
-    isidentifier = str.isidentifier
-
-    def stdin_get_value():
-        """Read the value from stdin."""
-        return TextIOWrapper(sys.stdin.buffer, errors='ignore').read()
+    except (LookupError, SyntaxError, UnicodeError):
+        # Fall back if file encoding is improperly declared
+        with open(filename, encoding='latin-1') as f:
+            return f.readlines()
+
+
+def stdin_get_value():
+    """Read the value from stdin."""
+    return io.TextIOWrapper(sys.stdin.buffer, errors='ignore').read()
+
 
 noqa = lru_cache(512)(re.compile(r'# no(?:qa|pep8)\b', re.I).search)
 
@@ -1784,16 +1734,8 @@ def expand_indent(line):
     r"""Return the amount of indentation.
 
     Tabs are expanded to the next multiple of 8.
-
-    >>> expand_indent('    ')
-    4
-    >>> expand_indent('\t')
-    8
-    >>> expand_indent('       \t')
-    8
-    >>> expand_indent('        \t')
-    16
     """
+    line = line.rstrip('\n\r')
     if '\t' not in line:
         return len(line) - len(line.lstrip())
     result = 0
@@ -1808,15 +1750,7 @@ def expand_indent(line):
 
 
 def mute_string(text):
-    """Replace contents with 'xxx' to prevent syntax matching.
-
-    >>> mute_string('"abc"')
-    '"xxx"'
-    >>> mute_string("'''abc'''")
-    "'''xxx'''"
-    >>> mute_string("r'abc'")
-    "r'xxx'"
-    """
+    """Replace contents with 'xxx' to prevent syntax matching."""
     # String modifiers (e.g. u or r)
     start = text.index(text[-1]) + 1
     end = len(text) - 1
@@ -1840,7 +1774,7 @@ def parse_udiff(diff, patterns=None, parent='.'):
             continue
         if line[:3] == '@@ ':
             hunk_match = HUNK_REGEX.match(line)
-            (row, nrows) = [int(g or '1') for g in hunk_match.groups()]
+            (row, nrows) = (int(g or '1') for g in hunk_match.groups())
             rv[path].update(range(row, row + nrows))
         elif line[:3] == '+++':
             path = line[4:].split('\t', 1)[0]
@@ -1901,7 +1835,7 @@ def _is_eol_token(token):
 ########################################################################
 
 
-class Checker(object):
+class Checker:
     """Load a Python source file, tokenize it, check coding style."""
 
     def __init__(self, filename=None, lines=None,
@@ -1916,8 +1850,11 @@ class Checker(object):
         self._ast_checks = options.ast_checks
         self.max_line_length = options.max_line_length
         self.max_doc_length = options.max_doc_length
+        self.indent_size = options.indent_size
+        self.fstring_start = 0
         self.multiline = False  # in a multiline string?
         self.hang_closing = options.hang_closing
+        self.indent_size = options.indent_size
         self.verbose = options.verbose
         self.filename = filename
         # Dictionary where a checker can store its custom state.
@@ -1931,9 +1868,9 @@ class Checker(object):
         elif lines is None:
             try:
                 self.lines = readlines(filename)
-            except IOError:
+            except OSError:
                 (exc_type, exc) = sys.exc_info()[:2]
-                self._io_error = '%s: %s' % (exc_type.__name__, exc)
+                self._io_error = f'{exc_type.__name__}: {exc}'
                 self.lines = []
         else:
             self.lines = lines
@@ -1958,7 +1895,7 @@ class Checker(object):
         else:
             offset = (1, 0)
         self.report_error(offset[0], offset[1] or 0,
-                          'E901 %s: %s' % (exc_type.__name__, exc.args[0]),
+                          f'E901 {exc_type.__name__}: {exc.args[0]}',
                           self.report_invalid_syntax)
 
     def readline(self):
@@ -2011,6 +1948,8 @@ class Checker(object):
                 continue
             if token_type == tokenize.STRING:
                 text = mute_string(text)
+            elif token_type == FSTRING_MIDDLE:  # pragma: >=3.12 cover
+                text = 'x' * len(text)
             if prev_row:
                 (start_row, start_col) = start
                 if prev_row != start_row:    # different row
@@ -2081,22 +2020,36 @@ class Checker(object):
             self.report_error(1, 0, 'E902 %s' % self._io_error, readlines)
         tokengen = tokenize.generate_tokens(self.readline)
         try:
+            prev_physical = ''
             for token in tokengen:
                 if token[2][0] > self.total_lines:
                     return
                 self.noqa = token[4] and noqa(token[4])
-                self.maybe_check_physical(token)
+                self.maybe_check_physical(token, prev_physical)
                 yield token
+                prev_physical = token[4]
         except (SyntaxError, tokenize.TokenError):
             self.report_invalid_syntax()
 
-    def maybe_check_physical(self, token):
+    def maybe_check_physical(self, token, prev_physical):
         """If appropriate for token, check current physical line(s)."""
         # Called after every token, but act only on end of line.
-        if _is_eol_token(token):
-            # Obviously, a newline token ends a single physical line.
-            self.check_physical(token[4])
-        elif token[0] == tokenize.STRING and '\n' in token[1]:
+
+        if token.type == FSTRING_START:  # pragma: >=3.12 cover
+            self.fstring_start = token.start[0]
+        # a newline token ends a single physical line.
+        elif _is_eol_token(token):
+            # if the file does not end with a newline, the NEWLINE
+            # token is inserted by the parser, but it does not contain
+            # the previous physical line in `token[4]`
+            if token.line == '':
+                self.check_physical(prev_physical)
+            else:
+                self.check_physical(token.line)
+        elif (
+                token.type == tokenize.STRING and '\n' in token.string or
+                token.type == FSTRING_END
+        ):
             # Less obviously, a string that contains newlines is a
             # multiline string, either triple-quoted or with internal
             # newlines backslash-escaped. Check every physical line in
@@ -2112,14 +2065,18 @@ class Checker(object):
             # - have to wind self.line_number back because initially it
             #   points to the last line of the string, and we want
             #   check_physical() to give accurate feedback
-            if noqa(token[4]):
+            if noqa(token.line):
                 return
+            if token.type == FSTRING_END:  # pragma: >=3.12 cover
+                start = self.fstring_start
+            else:
+                start = token.start[0]
+            end = token.end[0]
+
             self.multiline = True
-            self.line_number = token[2][0]
-            _, src, (_, offset), _, _ = token
-            src = self.lines[self.line_number - 1][:offset] + src
-            for line in src.split('\n')[:-1]:
-                self.check_physical(line + '\n')
+            self.line_number = start
+            for line_number in range(start, end):
+                self.check_physical(self.lines[line_number - 1] + '\n')
                 self.line_number += 1
             self.multiline = False
 
@@ -2142,7 +2099,7 @@ class Checker(object):
             token_type, text = token[0:2]
             if self.verbose >= 3:
                 if token[2][0] == token[3][0]:
-                    pos = '[%s:%s]' % (token[2][1] or '', token[3][1])
+                    pos = '[{}:{}]'.format(token[2][1] or '', token[3][1])
                 else:
                     pos = 'l.%s' % token[3][0]
                 print('l.%s\t%s\t%s\t%r' %
@@ -2169,7 +2126,7 @@ class Checker(object):
         return self.report.get_file_results()
 
 
-class BaseReport(object):
+class BaseReport:
     """Collect the results of the checks."""
 
     print_filename = False
@@ -2251,7 +2208,7 @@ class BaseReport(object):
 
     def print_benchmark(self):
         """Print benchmark numbers."""
-        print('%-7.2f %s' % (self.elapsed, 'seconds elapsed'))
+        print('{:<7.2f} {}'.format(self.elapsed, 'seconds elapsed'))
         if self.elapsed:
             for key in self._benchmark_keys:
                 print('%-7d %s per second (%d total)' %
@@ -2269,7 +2226,7 @@ class StandardReport(BaseReport):
     """Collect and print the results of the checks."""
 
     def __init__(self, options):
-        super(StandardReport, self).__init__(options)
+        super().__init__(options)
         self._fmt = REPORT_FORMAT.get(options.format.lower(),
                                       options.format)
         self._repeat = options.repeat
@@ -2279,13 +2236,12 @@ class StandardReport(BaseReport):
     def init_file(self, filename, lines, expected, line_offset):
         """Signal a new file."""
         self._deferred_print = []
-        return super(StandardReport, self).init_file(
+        return super().init_file(
             filename, lines, expected, line_offset)
 
     def error(self, line_number, offset, text, check):
         """Report an error, according to options."""
-        code = super(StandardReport, self).error(line_number, offset,
-                                                 text, check)
+        code = super().error(line_number, offset, text, check)
         if code and (self.counters[code] == 1 or self._repeat):
             self._deferred_print.append(
                 (line_number, offset, code, text[5:], check.__doc__))
@@ -2324,16 +2280,16 @@ class DiffReport(StandardReport):
     """Collect and print the results for the changed lines only."""
 
     def __init__(self, options):
-        super(DiffReport, self).__init__(options)
+        super().__init__(options)
         self._selected = options.selected_lines
 
     def error(self, line_number, offset, text, check):
         if line_number not in self._selected[self.filename]:
             return
-        return super(DiffReport, self).error(line_number, offset, text, check)
+        return super().error(line_number, offset, text, check)
 
 
-class StyleGuide(object):
+class StyleGuide:
     """Initialize a PEP-8 instance with few options."""
 
     def __init__(self, *args, **kwargs):
@@ -2360,8 +2316,7 @@ class StyleGuide(object):
             options.reporter = BaseReport if options.quiet else StandardReport
 
         options.select = tuple(options.select or ())
-        if not (options.select or options.ignore or
-                options.testsuite or options.doctest) and DEFAULT_IGNORE:
+        if not (options.select or options.ignore) and DEFAULT_IGNORE:
             # The default choice: ignore controversial checks
             options.ignore = tuple(DEFAULT_IGNORE.split(','))
         else:
@@ -2423,8 +2378,10 @@ class StyleGuide(object):
                     dirs.remove(subdir)
             for filename in sorted(files):
                 # contain a pattern that matches?
-                if ((filename_match(filename, filepatterns) and
-                     not self.excluded(filename, root))):
+                if (
+                    filename_match(filename, filepatterns) and
+                    not self.excluded(filename, root)
+                ):
                     runner(os.path.join(root, filename))
 
     def excluded(self, filename, parent=None):
@@ -2475,8 +2432,8 @@ def get_parser(prog='pycodestyle', version=__version__):
                           usage="%prog [options] input ...")
     parser.config_options = [
         'exclude', 'filename', 'select', 'ignore', 'max-line-length',
-        'max-doc-length', 'hang-closing', 'count', 'format', 'quiet',
-        'show-pep8', 'show-source', 'statistics', 'verbose']
+        'max-doc-length', 'indent-size', 'hang-closing', 'count', 'format',
+        'quiet', 'show-pep8', 'show-source', 'statistics', 'verbose']
     parser.add_option('-v', '--verbose', default=0, action='count',
                       help="print status messages, or debug with -vv")
     parser.add_option('-q', '--quiet', default=0, action='count',
@@ -2516,6 +2473,10 @@ def get_parser(prog='pycodestyle', version=__version__):
                       default=None,
                       help="set maximum allowed doc line length and perform "
                            "these checks (unchecked if not set)")
+    parser.add_option('--indent-size', type='int', metavar='n',
+                      default=INDENT_SIZE,
+                      help="set how many spaces make up an indent "
+                           "(default: %default)")
     parser.add_option('--hang-closing', action='store_true',
                       help="hang closing bracket instead of matching "
                            "indentation of opening bracket's line")
@@ -2525,11 +2486,6 @@ def get_parser(prog='pycodestyle', version=__version__):
                       help="report changes only within line number ranges in "
                            "the unified diff received on STDIN")
     group = parser.add_option_group("Testing Options")
-    if os.path.exists(TESTSUITE_PATH):
-        group.add_option('--testsuite', metavar='dir',
-                         help="run regression tests from dir")
-        group.add_option('--doctest', action='store_true',
-                         help="run doctest on myself")
     group.add_option('--benchmark', action='store_true',
                      help="measure processing speed")
     return parser
@@ -2546,7 +2502,7 @@ def read_config(options, args, arglist, parser):
     merged together (in that order) using the read method of
     ConfigParser.
     """
-    config = RawConfigParser()
+    config = configparser.RawConfigParser()
 
     cli_conf = options.config
 
@@ -2590,8 +2546,8 @@ def read_config(options, args, arglist, parser):
                 print("  unknown option '%s' ignored" % opt)
                 continue
             if options.verbose > 1:
-                print("  %s = %s" % (opt,
-                                     config.get(pycodestyle_section, opt)))
+                print("  {} = {}".format(opt,
+                                         config.get(pycodestyle_section, opt)))
             normalized_opt = opt.replace('-', '_')
             opt_type = option_list[normalized_opt]
             if opt_type in ('int', 'count'):
@@ -2606,7 +2562,6 @@ def read_config(options, args, arglist, parser):
 
         # Third, overwrite with the command-line options
         (options, __) = parser.parse_args(arglist, values=new_options)
-    options.doctest = options.testsuite = False
     return options
 
 
@@ -2639,17 +2594,14 @@ def process_options(arglist=None, parse_argv=False, config_file=None,
     if verbose is not None:
         options.verbose = verbose
 
-    if options.ensure_value('testsuite', False):
-        args.append(options.testsuite)
-    elif not options.ensure_value('doctest', False):
-        if parse_argv and not args:
-            if options.diff or any(os.path.exists(name)
-                                   for name in PROJECT_CONFIG):
-                args = ['.']
-            else:
-                parser.error('input not specified')
-        options = read_config(options, args, arglist, parser)
-        options.reporter = parse_argv and options.quiet == 1 and FileReport
+    if parse_argv and not args:
+        if options.diff or any(os.path.exists(name)
+                               for name in PROJECT_CONFIG):
+            args = ['.']
+        else:
+            parser.error('input not specified')
+    options = read_config(options, args, arglist, parser)
+    options.reporter = parse_argv and options.quiet == 1 and FileReport
 
     options.filename = _parse_multi_options(options.filename)
     options.exclude = normalize_paths(options.exclude)
@@ -2694,11 +2646,7 @@ def _main():
     style_guide = StyleGuide(parse_argv=True)
     options = style_guide.options
 
-    if options.doctest or options.testsuite:
-        from testsuite.support import run_tests
-        report = run_tests(style_guide)
-    else:
-        report = style_guide.check_files()
+    report = style_guide.check_files()
 
     if options.statistics:
         report.print_statistics()
@@ -2706,9 +2654,6 @@ def _main():
     if options.benchmark:
         report.print_benchmark()
 
-    if options.testsuite and not options.quiet:
-        report.print_results()
-
     if report.total_errors:
         if options.count:
             sys.stderr.write(str(report.total_errors) + '\n')
diff --git a/tests/clean1.py b/tests/clean1.py
index 3147046..43f5b99 100644
--- a/tests/clean1.py
+++ b/tests/clean1.py
@@ -5,6 +5,7 @@ This Python module should pass ``pylint`` cleanly.
 
 """
 
+
 def get(msg, key):
     """ Get property value.
 
@@ -19,3 +20,7 @@ def get(msg, key):
         if prop.key == key:
             return prop.value
     return None
+
+
+def colon_in_fstring(hoge, foo):
+    print(f"{hoge}:{foo}")
